{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.data_utils import fetch_batch_raw_data\n",
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2025-05-10 08:00:00+00:00\n",
      "Fetching data from: 2025-04-11 08:00:00+00:00\n",
      "Fetching data to: 2025-05-10 08:00:00+00:00\n",
      "File already exists for 2024-12.\n",
      "Loading data for 2024-12...\n",
      "Before filtering - 2024-12: 2311171 rows\n",
      "After duration filter - 2024-12: 2308841 rows\n",
      "After station filter - 2024-12: 2310051 rows\n",
      "After date range filter - 2024-12: 2310900 rows\n",
      "Total records: 2,311,171\n",
      "Valid records: 2,307,489\n",
      "Records dropped: 3,682 (0.16%)\n",
      "Successfully processed data for 2024-12.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "File already exists for 2025-01.\n",
      "Loading data for 2025-01...\n",
      "Before filtering - 2025-01: 2124475 rows\n",
      "After duration filter - 2025-01: 2122991 rows\n",
      "After station filter - 2025-01: 2123911 rows\n",
      "After date range filter - 2025-01: 2124268 rows\n",
      "Total records: 2,124,475\n",
      "Valid records: 2,122,262\n",
      "Records dropped: 2,213 (0.10%)\n",
      "Successfully processed data for 2025-01.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "Inspecting rides DataFrame before transformation:\n",
      "               pickup_datetime                ended_at start_station_name  \\\n",
      "276274 2025-04-11 08:08:40.607 2024-12-20 08:22:11.352   1 Ave & E 110 St   \n",
      "219889 2025-04-11 08:21:13.175 2024-12-20 08:45:27.124   1 Ave & E 110 St   \n",
      "528692 2025-04-11 08:30:40.045 2024-12-20 08:35:40.580   1 Ave & E 110 St   \n",
      "306498 2025-04-11 08:32:42.925 2024-12-20 08:34:58.502   1 Ave & E 110 St   \n",
      "17781  2025-04-11 08:39:44.583 2024-12-20 08:59:39.757   1 Ave & E 110 St   \n",
      "\n",
      "                     duration  \n",
      "276274 0 days 00:13:30.745000  \n",
      "219889 0 days 00:24:13.949000  \n",
      "528692 0 days 00:05:00.535000  \n",
      "306498 0 days 00:02:15.577000  \n",
      "17781  0 days 00:19:55.174000  \n",
      "Number of rows in rides: 1780665\n",
      "pickup_datetime range: 2025-04-11 08:00:00.313000 to 2025-05-10 07:59:58.799000\n",
      "Unique start_station_name values: 2152\n",
      "pickup_datetime dtype: datetime64[ns]\n",
      "start_station_name dtype: object\n",
      "Sample start_station_name values: ['1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St']\n",
      "After creating pickup_hour:\n",
      "pickup_hour range: 2025-04-11 08:00:00 to 2025-05-10 07:00:00\n",
      "Any NaN in pickup_hour: 0\n",
      "Any NaN in start_station_name: 0\n",
      "pickup_hour dtype: datetime64[ns]\n",
      "After groupby().size():\n",
      "Number of groups: 596067\n",
      "Sample groups:\n",
      "pickup_hour          start_station_name\n",
      "2025-04-11 08:00:00  1 Ave & E 110 St       8\n",
      "                     1 Ave & E 118 St       4\n",
      "                     1 Ave & E 16 St        7\n",
      "                     1 Ave & E 18 St       25\n",
      "                     1 Ave & E 30 St        9\n",
      "dtype: int64\n",
      "After filtering to top 3 stations:\n",
      "Number of rows: 2088\n",
      "Unique start_station_name values: 3\n",
      "Stations retained: <StringArray>\n",
      "['11 Ave & W 41 St', 'W 21 St & 6 Ave', 'W 31 St & 7 Ave']\n",
      "Length: 3, dtype: string\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2088 entries, 0 to 2087\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   pickup_hour         2088 non-null   datetime64[ns]\n",
      " 1   start_station_name  2088 non-null   string        \n",
      " 2   rides               2088 non-null   int16         \n",
      "dtypes: datetime64[ns](1), int16(1), string(1)\n",
      "memory usage: 36.8 KB\n",
      "2025-05-10 04:41:48,801 INFO: Initializing external client\n",
      "2025-05-10 04:41:48,802 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.1.5 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 04:41:49,752 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1225907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2088/2088 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: recent_time_series_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1225907/jobs/named/recent_time_series_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('recent_time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.data_utils import fetch_batch_raw_data\n",
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "import src.config as config\n",
    "\n",
    "# Determine the current date and time (floored to the hour)\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor(\"h\")\n",
    "print(f\"Current date: {current_date}\")\n",
    "\n",
    "# Define the date range for fetching data (last 29 days)\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "print(f\"Fetching data from: {fetch_data_from}\")\n",
    "print(f\"Fetching data to: {fetch_data_to}\")\n",
    "\n",
    "# Fetch historical data (shifted back 16 weeks)\n",
    "rides = fetch_batch_raw_data(fetch_data_from, fetch_data_to)\n",
    "rides.head(5)\n",
    "\n",
    "# Transform raw data into time series format\n",
    "ts_data = transform_raw_data_into_ts_data(rides)\n",
    "ts_data.head(5)\n",
    "ts_data.info()\n",
    "\n",
    "# Connect to Hopsworks and upload the data to a new feature group\n",
    "import hopsworks\n",
    "\n",
    "# Connect to the project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# Connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Connect to or create a new feature group for recent data\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=\"recent_time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Recent time series data at hourly frequency for inference\",\n",
    "    primary_key=[\"start_station_name\", \"pickup_hour\"],\n",
    "    event_time=\"pickup_hour\"\n",
    ")\n",
    "\n",
    "# Upload the time series data to the feature group\n",
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting current date, and setting timelines for data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2025-05-10 08:00:00+00:00\n",
      "Fetching data from: 2025-04-11 08:00:00+00:00\n",
      "Fetching data to: 2025-05-10 08:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Determine the current date and time (floored to the hour)\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor(\"h\")\n",
    "print(f\"Current date: {current_date}\")\n",
    "\n",
    "# Define the date range for fetching data (last 29 days)\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "print(f\"Fetching data from: {fetch_data_from}\")\n",
    "print(f\"Fetching data to: {fetch_data_to}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition for fetching data in a batch for specified timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for 2024-12.\n",
      "Loading data for 2024-12...\n",
      "Before filtering - 2024-12: 2311171 rows\n",
      "After duration filter - 2024-12: 2308841 rows\n",
      "After station filter - 2024-12: 2310051 rows\n",
      "After date range filter - 2024-12: 2310900 rows\n",
      "Total records: 2,311,171\n",
      "Valid records: 2,307,489\n",
      "Records dropped: 3,682 (0.16%)\n",
      "Successfully processed data for 2024-12.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "File already exists for 2025-01.\n",
      "Loading data for 2025-01...\n",
      "Before filtering - 2025-01: 2124475 rows\n",
      "After duration filter - 2025-01: 2122991 rows\n",
      "After station filter - 2025-01: 2123911 rows\n",
      "After date range filter - 2025-01: 2124268 rows\n",
      "Total records: 2,124,475\n",
      "Valid records: 2,122,262\n",
      "Records dropped: 2,213 (0.10%)\n",
      "Successfully processed data for 2025-01.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "Inspecting rides DataFrame before transformation:\n",
      "               pickup_datetime                ended_at start_station_name  \\\n",
      "276274 2025-04-11 08:08:40.607 2024-12-20 08:22:11.352   1 Ave & E 110 St   \n",
      "219889 2025-04-11 08:21:13.175 2024-12-20 08:45:27.124   1 Ave & E 110 St   \n",
      "528692 2025-04-11 08:30:40.045 2024-12-20 08:35:40.580   1 Ave & E 110 St   \n",
      "306498 2025-04-11 08:32:42.925 2024-12-20 08:34:58.502   1 Ave & E 110 St   \n",
      "17781  2025-04-11 08:39:44.583 2024-12-20 08:59:39.757   1 Ave & E 110 St   \n",
      "\n",
      "                     duration  \n",
      "276274 0 days 00:13:30.745000  \n",
      "219889 0 days 00:24:13.949000  \n",
      "528692 0 days 00:05:00.535000  \n",
      "306498 0 days 00:02:15.577000  \n",
      "17781  0 days 00:19:55.174000  \n",
      "Number of rows in rides: 1780665\n",
      "pickup_datetime range: 2025-04-11 08:00:00.313000 to 2025-05-10 07:59:58.799000\n",
      "Unique start_station_name values: 2152\n",
      "pickup_datetime dtype: datetime64[ns]\n",
      "start_station_name dtype: object\n",
      "Sample start_station_name values: ['1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St', '1 Ave & E 110 St']\n",
      "After creating pickup_hour:\n",
      "pickup_hour range: 2025-04-11 08:00:00 to 2025-05-10 07:00:00\n",
      "Any NaN in pickup_hour: 0\n",
      "Any NaN in start_station_name: 0\n",
      "pickup_hour dtype: datetime64[ns]\n",
      "After groupby().size():\n",
      "Number of groups: 596067\n",
      "Sample groups:\n",
      "pickup_hour          start_station_name\n",
      "2025-04-11 08:00:00  1 Ave & E 110 St       8\n",
      "                     1 Ave & E 118 St       4\n",
      "                     1 Ave & E 16 St        7\n",
      "                     1 Ave & E 18 St       25\n",
      "                     1 Ave & E 30 St        9\n",
      "dtype: int64\n",
      "After filtering to top 3 stations:\n",
      "Number of rows: 2088\n",
      "Unique start_station_name values: 3\n",
      "Stations retained: <StringArray>\n",
      "['11 Ave & W 41 St', 'W 21 St & 6 Ave', 'W 31 St & 7 Ave']\n",
      "Length: 3, dtype: string\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2088 entries, 0 to 2087\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   pickup_hour         2088 non-null   datetime64[ns]\n",
      " 1   start_station_name  2088 non-null   string        \n",
      " 2   rides               2088 non-null   int16         \n",
      "dtypes: datetime64[ns](1), int16(1), string(1)\n",
      "memory usage: 36.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Fetch historical data (shifted back 16 weeks)\n",
    "rides = fetch_batch_raw_data(fetch_data_from, fetch_data_to)\n",
    "rides.head(5)\n",
    "\n",
    "# Transform raw data into time series format\n",
    "ts_data = transform_raw_data_into_ts_data(rides)\n",
    "ts_data.head(5)\n",
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopsworks Login and loading feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 04:42:15,085 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-05-10 04:42:15,089 INFO: Initializing external client\n",
      "2025-05-10 04:42:15,090 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.1.5 may not be compatible with the connected Hopsworks backend version 4.2.0. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 04:42:15,644 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1225907\n"
     ]
    }
   ],
   "source": [
    "# Connect to Hopsworks and upload the data\n",
    "import hopsworks\n",
    "\n",
    "# Connect to the project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# Connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Connect to the feature group\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    "    description=\"Time series data at hourly frequency\",\n",
    "    primary_key=[\"start_station_name\", \"pickup_hour\"],\n",
    "    event_time=\"pickup_hour\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2088/2088 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citi_bike_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1225907/jobs/named/citi_bike_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('citi_bike_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the time series data to the feature group\n",
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CDA500PF2)",
   "language": "python",
   "name": "cda500pf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
